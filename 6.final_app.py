import streamlit as st
from dotenv import load_dotenv
import os
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.messages import HumanMessage, AIMessage
import uuid
import datetime

# API KEY ì •ë³´ ë¡œë“œ
load_dotenv()


# ë²¡í„°ìŠ¤í† ì–´ ë¶ˆëŸ¬ì˜¤ê¸°
@st.cache_resource
def load_vectorstore():
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
        dimensions=1024
    )
    db = FAISS.load_local("my_faiss_index",
                          embeddings,
                          allow_dangerous_deserialization=True)
    return db


db = load_vectorstore()

# ì‹œê°„ ì •ë³´ ì…ë ¥ì„ ìœ„í•´ í•¨ìˆ˜ êµ¬í˜„


def time_now():
    return datetime.datetime.now().strftime('%Yë…„%mì›”%dì¼ %Hì‹œ%Më¶„%Sì´ˆ')


# LLM ì´ˆê¸°í™”
llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0)

# ê¸°ì¡´ ëŒ€í™” ê¸°ë¡ê³¼ RAGë¥¼ ê²°í•©í•œ ì‘ë‹µìƒì„± í•¨ìˆ˜


def generate_response_with_memory(question, chat_history, retriever):
    # RAG
    docs = retriever.get_relevant_documents(question)
    context = "\n\n".join([doc.page_content for doc in docs])

    # ì‹œê°„ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    current_time = time_now()

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— contextì™€ ëŒ€í™” ê¸°ì–µ ì§€ì‹œ
    messages = [
        ("system", f"""ë‹¹ì‹ ì€ ì¬í…Œí¬ ê´€ë ¨ ì§€ì‹ì´ í’ë¶€í•œ Question-Answering ì±—ë´‡ì…ë‹ˆë‹¤.
         ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì™€ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ëª¨ë‘ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.(ì»¨í…ìŠ¤íŠ¸ëŠ” ê³¼ê±°ì— ì‘ì„±ë˜ì–´ìˆìœ¼ë¯€ë¡œ ì˜¤ëŠ˜ ë‚ ì§œê°€ ì–´ë–»ê²Œ ë˜ëŠ”ì§€ëŠ” í—·ê°ˆë ¤í•˜ì§€ ì•Šê¸°.)
         ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì§œ í™•ì¸ì´ ê°€ëŠ¥í•˜ë‹¤ë©´ ê°€ì¥ ìµœê·¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•˜ì„¸ìš”.
         ì´ì „ ëŒ€í™”ì—ì„œ ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ê°œì¸ì •ë³´(ë‚˜ì´, ìƒí™© ë“±), ê´€ì‹¬ë¶„ì•¼ ë“±ì„ ê¸°ì–µí•˜ê³  í™œìš©í•˜ì„¸ìš”.

         í˜„ì¬ ì‹œê°„: {current_time}
         ì‚¬ìš©ìê°€ "ì§€ê¸ˆ ëª‡ì‹œì•¼?", "ì˜¤ëŠ˜ì´ ëª‡ì¼ì´ì•¼?" ê°™ì€ ì‹œê°„ ê´€ë ¨ ì§ˆë¬¸ì„ í•˜ë©´ ìœ„ì˜ í˜„ì¬ ì‹œê°„ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.

         ì»¨í…ìŠ¤íŠ¸: {context}""")
    ]

    # ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ëª¨ë‘ ë©”ì„¸ì§€ì— ì¶”ê°€
    for q, a in chat_history:
        messages.append(("human", q))
        messages.append(("assistant", a))

    # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
    messages.append(("human", question))

    # LLMì— ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬
    response = llm.invoke(messages)

    return response.content


# streamlit êµ¬í˜„
st.title("ğŸ¦ğŸ’¸ì¬í…Œí¬ ì „ëµ ë„ìš°ë¯¸ ChatbotğŸ§ ")

# ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ session stateì— ì €ì¥
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []

# ëŒ€í™” ê¸°ë¡ ì‚­ì œ
if st.button("ëŒ€í™” ë‚´ì—­ ì‚­ì œ", type="primary"):
    st.session_state["chat_history"] = []
    st.rerun()

# ì±„íŒ… ì‹¤í–‰ ë²„íŠ¼(Enter í‚¤ ê°€ëŠ¥)
with st.form("chat_form", clear_on_submit=True):
    user_input = st.text_input(
        "ë©”ì„¸ì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”", key="user_input", placeholder="ë©”ì„¸ì§€ ì…ë ¥ í›„ Enter")
    submitted = st.form_submit_button("ì „ì†¡")

if submitted and user_input.strip():
    with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
        try:
            # ëŒ€í™” ê¸°ë¡ê³¼ RAG ê²°í•©í•œ ì‘ë‹µ ìƒì„±
            answer = generate_response_with_memory(
                user_input,
                st.session_state["chat_history"],
                db.as_retriever()
            )

            st.session_state["chat_history"].append((user_input, answer))
        except Exception as e:
            st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.:{str(e)}")


# ëŒ€í™” ê¸°ë¡
st.subheader("your chat")
for i, (q, a) in enumerate(reversed(st.session_state["chat_history"])):
    real_idx = len(st.session_state["chat_history"]) - 1 - i
    col1, col2 = st.columns([7, 1])
    with col1:
        st.markdown(f"**ì§ˆë¬¸:** {q}")
        st.markdown(f"**ë‹µë³€:** {a}")
    with col2:
        if st.button("ğŸ—‘ï¸", key=f"delete_{real_idx}"):
            st.session_state["chat_history"].pop(real_idx)
            st.rerun()
    st.markdown("---")
