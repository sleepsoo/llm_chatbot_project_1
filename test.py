import streamlit as st
from dotenv import load_dotenv
import os
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.messages import HumanMessage, AIMessage
import uuid

# API KEY ì •ë³´ ë¡œë“œ
load_dotenv()

# ë²¡í„°ìŠ¤í† ì–´ ë¶ˆëŸ¬ì˜¤ê¸°


@st.cache_resource
def load_vectorstore():
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-small",
        dimensions=1024
    )
    db = FAISS.load_local("my_faiss_index",
                          embeddings,
                          allow_dangerous_deserialization=True)
    return db


db = load_vectorstore()

# LLM ì´ˆê¸°í™”
llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0)

# ëŒ€í™” ê¸°ë¡ê³¼ RAGë¥¼ ê²°í•©í•œ ì‘ë‹µ ìƒì„± í•¨ìˆ˜


def generate_response_with_memory(question, chat_history, retriever):
    # 1. RAG: ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    docs = retriever.get_relevant_documents(question)
    context = "\n\n".join([doc.page_content for doc in docs])

    # 2. ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë©”ì‹œì§€ í˜•íƒœë¡œ êµ¬ì„±
    messages = [
        ("system", f"""ë‹¹ì‹ ì€ ì¬í…Œí¬ ê´€ë ¨ ì§€ì‹ì´ í’ë¶€í•œ Question-Answering ì±—ë´‡ì…ë‹ˆë‹¤.

        ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì™€ ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ëª¨ë‘ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
        ì´ì „ ëŒ€í™”ì—ì„œ ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ê°œì¸ì •ë³´(ë‚˜ì´, ìƒí™© ë“±)ë¥¼ ê¸°ì–µí•˜ê³  í™œìš©í•˜ì„¸ìš”.

        ì»¨í…ìŠ¤íŠ¸:
        {context}""")
    ]

    # 3. ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶”ê°€
    for q, a in chat_history:
        messages.append(("human", q))
        messages.append(("assistant", a))

    # 4. í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
    messages.append(("human", question))

    # 5. LLM í˜¸ì¶œ
    response = llm.invoke(messages)

    return response.content


# Streamlit êµ¬í˜„
st.title("ğŸ¦ğŸ’¸ì¬í…Œí¬ ì „ëµ ë„ìš°ë¯¸ ChatbotğŸ§ ")

# ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ session stateì— ì €ì¥
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []

# ëŒ€í™” ê¸°ë¡ ì‚­ì œ
if st.button("ëŒ€í™” ë‚´ì—­ ì‚­ì œ", type="primary"):
    st.session_state["chat_history"] = []
    st.rerun()

# ì±„íŒ… ì‹¤í–‰ ë²„íŠ¼(Enter í‚¤ ê°€ëŠ¥)
with st.form("chat_form", clear_on_submit=True):
    user_input = st.text_input(
        "ë©”ì„¸ì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”", key="user_input", placeholder="ë©”ì„¸ì§€ ì…ë ¥ í›„ Enter")
    submitted = st.form_submit_button("ì „ì†¡")

if submitted and user_input.strip():
    with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
        try:
            # ëŒ€í™” ê¸°ë¡ê³¼ RAGë¥¼ ê²°í•©í•œ ì‘ë‹µ ìƒì„±
            answer = generate_response_with_memory(
                user_input,
                st.session_state["chat_history"],
                db.as_retriever()
            )

            # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
            st.session_state["chat_history"].append((user_input, answer))

        except Exception as e:
            st.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

# ëŒ€í™” ê¸°ë¡
st.subheader("ëŒ€í™” ê¸°ë¡")
for i, (q, a) in enumerate(reversed(st.session_state["chat_history"])):
    real_idx = len(st.session_state["chat_history"]) - 1 - i
    col1, col2 = st.columns([7, 1])
    with col1:
        st.markdown(f"**ì§ˆë¬¸:** {q}")
        st.markdown(f"**ë‹µë³€:** {a}")
    with col2:
        if st.button("ğŸ—‘ï¸", key=f"delete_{real_idx}"):
            st.session_state["chat_history"].pop(real_idx)
            st.rerun()
    st.markdown("---")

# ë””ë²„ê·¸ìš©: í˜„ì¬ ì €ì¥ëœ ëŒ€í™” ê¸°ë¡ í™•ì¸
if st.checkbox("ë””ë²„ê·¸: ì €ì¥ëœ ëŒ€í™” ê¸°ë¡ ë³´ê¸°"):
    st.write("í˜„ì¬ ì €ì¥ëœ ëŒ€í™” ìˆ˜:", len(st.session_state["chat_history"]))
    for i, (q, a) in enumerate(st.session_state["chat_history"]):
        st.write(f"{i+1}. Q: {q[:50]}...")
        st.write(f"   A: {a[:50]}...")
